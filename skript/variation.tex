\chapter{Variationsrechnung\label{chapter-variationsrechnung}}

\section{Was ist Variationsrechnung?}
Alle bisher betrachteten Optimierungsproblem gingen davon aus, dass
ein Vektor $x$, also endlich viele reelle Variablen, gefunden werden muss,
der eine Funktion $f(x)$ maximieren soll, aber in einem vorgegebenen
Gebiet von zul"assigen Vektoren liegen muss. 
Nat"urlich kann die Zahl der Variablen in so einem Problem sehr gross werden,
sie bleibt aber immer endlich.

In diesen Rahmen passt zum Beispiel das Problem, ein Polygon
maximalen Fl"acheninhaltes zu finden. Solange die Zahl der Ecken
des Polygons beschr"ankt bleibt, l"asst sich ein Polygon immer mit
endlich vielen Variablen beschreiben. Sucht man aber nicht nur in
der Menge der Polygone, sondern eine Kurve vorgegebener L"ange,
die maximalen Fl"acheninhalt einschliessen soll, dann kann man
die L"osung nicht mehr mit endlich vielen Variablen beschreiben.
Gesucht ist jetzt eine Funktion, nicht nur ein Vektor.

\subsection{Eine historische Herausforderung: Brachistochrone}
Im Juni 1696 ver"offentlichte Johann Bernoulli (1667-1748), der
zu dieser Zeit Professor f"ur Mathematik und Medizin in Groningen
war, den folgende Aufgabe in der Zeitschrift {\it Acta Eruditorum}:

\begin{quote}
Wenn in einer verticalen Ebene zwei Punkte $A$ und $B$ gegeben sind,
soll man dem beweglichen Punkte $M$ eine Bahn $AMB$ anweisen, auf welcher
er von $A$ ausgehend verm"oge seiner eigenen Schwere in k"urzester Zeit
nach $B$ gelangt.
\end{quote}

Offenbar wollte er auch etwas damit angeben, dass er das Problem
schon gel"ost hatte, und schrieb daher weiter
\begin{quote}
Damit Liebhaber solcher Dinge Lust bekommen sich an die L"osung
dieses Problems zu wagen, m"ogen sie wissen, dass es nicht, wie es
scheinen k"onnte, blosse Speculation ist und keinen praktischen
Nutzen hat. Vielmehr erweist es sich sogar, was man kaum glauben
sollte, auch f"ur andere Wissenszweige, als die Mechanik, sehr
n"utzlich. Um einem voreiligen Urtheile entgegenzutreten, m"oge noch
bemerkt werden, dass die gerade Linie $AB$ zwar die k"urzeste zwischen
$A$ und $B$ ist, jedoch nicht in k"urzester Zeit durchlaufen wird. Wohl
aber ist die Curve $AMB$ eine den Geometern sehr bekannte; die ich
angeben werde, wenn sie nach Verlauf dieses Jahres kein anderer
genannt hat.
\end{quote}
Leibniz reagiert postwendend mit einer L"osung, sein Antwortbrief tr"agt
das Datum 16. Juni 1696. Leibniz wies auch darauf hin, dass angesichts
der damals noch nicht so schnellen Postdienste die Antwortfrist noch
etwas verl"angert werden sollte.
Bernoulli publiziert daher im Januar 1697 nochmals einen nicht
minder schw"ulstigen Aufruf
\begin{quote}
Die scharfsinnigsten Mathematiker des ganzen Erdkreises gr"usst
Johann Bernoulli, "offentlicher Professor der Mathematik.
Da die Erfahrung zeigt, dass edle Geister zur Arbeit an der Vermehrung
des Wissens durch nichts mehr angetrieben werden, als wenn man ihnen
schwierige und zugleich n"utzliche Aufgaben vorlegt, durch deren
L"osung sie einen ber"uhmten Namen erlangen und sich bei der Nachwelt
ein ewiges Denkmal setzen, so hoffte ich den Dank der mathematischen
Welt zu verdienen, wenn ich nach dem Beispiele von M"annern wie
Mersenne, Pascal, Fermat, Viviani und anderen, welche vor mir
dasselbe thaten, den ausgezeichnetsten Analysten dieser Zeit eine
Aufgabe vorlegte, damit sie daran, wie an einem Pr"ufsteine, die
G"ute ihrer Methoden beurtheilen, ihre Kr"afte erproben und, wenn sie
etwas f"anden, mir mittheilen k"onnten; dann w"urde einem jeden
"offentlich sein verdientes Lob von mir zu Theil geworden sein.
\end{quote}
Im Mai-Heft 1697 der Acta Eruditorum wurden dann L"osungen von
Johann Bernoulli, seinem "alteren Bruder Jakob Bernoulli, von Marquis
de l'H\^opital (1661-1704) und
von Ehrenfried Walter Graf von Tschirnhausen (1651-1708)
ver"offentlicht.
Die L"osung von Leibniz wurde nicht ver"offentlich, Leibniz
meinte, seine L"osung sei so "ahnlich der L"osung der Bernoulli-Br"uder,
dass eine Publikation nicht nahel"age. Leibniz wies aber auch darauf hin,
dass Huygens, der 1695 verstorben war, das Problem sicher auch h"atte l"osen
k"onnen. Ebenso Newton.
Newton war mit der Analysis seit l"angerer Zeit im Besitz einer Methode,
mit der genau solche Aufgaben gel"ost werden konnten.
Allerdings ver"offentlichte er seine Methode erst 1704, was ihn
aber nicht daran hinderte, sie zum Beispiel zur L"osung des
Brachistochronen-Problems zu verwenden, die er anonym und unabh"angig von
den Acta Eruditorum in der Januar-Ausgabe der {\it Philosophical Transactions}.
Zu dieser Zeit galt Leibniz als der Erfinder der Infinitesimalrechnung,
mit der er sich seit 1670 besch"aftigt hatte.

Das Brachistochronen-Problem ist offenbar ein Variationsproblem.
Gesucht ist eine Kurve so, die von $A$ nach $B$ f"uhrt so, dass
die zum durchlaufen der Kurve n"otige Zeit m"oglichst kurz ist.

\subsection{Problemstellung}
Ein Variationsproblem ist also eine spezielle Art eines nichtlinearen
Optimierungsproblems, in dem nicht nur ein endlichdimensionaler
Vektor gefunden werden muss, sondern eine Funktion $[a,b]\to \mathbb R^n$.
Der linearen Zielfunktion $c^tx$ entspr"ache am ehesten ein Integral
\[
I(x)=\int_a^b c(t) x(t)\,dt.
\]
Wir haben aber im Kapitel \ref{chapter-lineare-optimierung} gesehen haben,
ist die L"osung eines linearen Optimierungsproblems vor allem eine 
Funktion des zul"assigen Bereichs.
Erst eine nichtlineare Zielfunktion f"uhrt zu einer interessanten
Theorie. Eine Zielfunktion der Form
\[
I(x)=\int_a^b F(t, x(t))\,dt.
\]
ist also angemessener.
Schon das Brachistochronen-Problem zeigt
jedoch, dass nicht nur die Funktionswerte, sondern auch die Werte
der Ableitung eine Rolle spielen k"onnen, und gelangen damit
zu Zielfunktion, die wir verwenden wollen:
\begin{equation}
I(x)=\int_a^b F(t, x(t), \dot x(t))\,dt,
\label{variations-funktional}
\end{equation}
wobei $F(t,x,v)$ eine stetige Funktion ist.

Grunds"atzlich spricht nichts dagegen, noch kompliziertere
Funktionen zuzulassen, die zum Beispiel auch noch h"ohere
Ableitungen ber"ucksichtigen.
Die Variationsproblem der Form (\ref{variations-funktional}) 
haben jedoch eine allgemeine L"osung, auf die wir im Folgenden
hinarbeiten wollen.

\subsection{Verallgemeinerungen}

\subsection{Beispiele}
\subsubsection{K"urzeste Verbindung}
Man finde die k"urzeste Verbindung zweier Punkte $x_0$ und $x_1$.
Offenbar muss hierzu die Kurvenl"ange
\begin{equation}
l(x)=\int_0^1 \sqrt{1+\dot x(t)^2}\,dt
\label{kuerzeste-verbindung-variationsprinzip}
\end{equation}
minimiert werden unter den Nebenbedingungen $x(0)=x_0$ und
$x(1)=x_1$.

\subsubsection{Isoperimetrisches Problem}
\subsubsection{Mechanik als Minimalproblem}
Vielen Naturgesetzen liegen Optimalit"atsprinzipien zu Grunde.
Das Brechungsgesetz kann man so formulieren: auf dem Weg $A$ 
nach $B$ nimmt Licht immer den schnellsten Weg.
Lagrange hat als erster erkannt, dass die Mechanik als Optimierungsproblem
verstanden werden kann. Doch welche Zielfunktion wird hier optimiert?

Wir m"ochten zum Beispiel die Bewegung eines Massepunktes in einem
Potential $V(x)$ verstehen, zum Beispiel die Bewegung eines Planeten
im Schwerefeld der Sonne.
Wir suchen also die Bahn $x(t)$ des Massepunktes finden. 
Wir erwarten, dass Position und Bewegungszustand des Massepunktes,
also $x(t)$ und $\dot x(t)$ wesentlich sein werden.
Wir vermuten also, dass es eine Funktion $L(t,x,v)$ gibt,
so dass das Integral
\begin{equation}
\int_{t_0}^{t_1} L(t, x(t), \dot x(t))\,dt
\label{lagrange-integral}
\end{equation}
durch die tats"achliche Bahnkurve von $x(t_0)$ nach $x(t_1)$
minimiert wird.

Intuitiv wissen wir, dass eine stabile Lage eines mechanischen Systems
sich durch minimale Energie auszeichnet: befindet sich ein System nicht
im Zustand minimaler Energie, kann man es durch eine kleine 
St"orung m"oglicherweise dazu bringen, seinen Zustand zu "andern
und dabei Energie abzugeben. Allerdings kann die Energie eines
einzelnen Massepunktes diese Funktion nicht "ubernehmen, da sie
konstant ist. Das Prinzip, dass ein System den Zustand kleinster
Energie annimmt, ist genauer betrachtet eine Bedingung an die potentielle
Energie. Je kleiner die potentielle Energie, und je schneller dieser
Zustand erreicht wird, desto besser. Als Extremalprinzip bietet
sich daher an dass die Differenz von potentieller und kinetischer
Energie minimiert werden soll. Wir setzen also
\begin{equation}
L(t,x,v)=V(x)-\frac12mv^2.
\label{lagrange-funktion}
\end{equation}
Diese Funktion $L(t,x,v)$ heisst Lagrange-Funktion des mechanischen
Systems, wir
verwenden sie im Funktional (\ref{lagrange-integral}).

\subsubsection{Interpolation}
\subsubsection{Minimalfl"achen}

\section{Euler-Gleichungen}
Es gibt kaum Verfahren, ein allgemeines Variationsproblem zu l"osen.
Aber es gibt ein allgemeines Vorgehen, wie man ein Variationsproblem
in eine Differentialgleichung umwandeln kann. F"ur Differentialgleichungen
steht ja eine grosses Arsenal von L"osungstechniken.
\subsection{Variation}
Das Maximum einer Funktion findet man, indem man diejenigen
Punkte sucht, in deren unmittelbarer Umgebung die Funktion kleinere
Werte annimmt. Erkennen kann ma solche Punkte daran, dass die Ableitung
der Funktion dort verschwindet.

"Ubertragen auf das Variationsproblem muss man also Funktionen 
suchen, die der L"osungsfunktion $x(t)$ benachbart sind.
Eine benachbarte Funktion kann zum Beispiel von der Form
$x(t)+s\delta(t)$ sein, wobei wir uns die Funktion $\delta(t)$
als kleine Abweichung vorstellen, die wir mit dem Parameter
$s$ noch kleiner machen k"onnen. 

Nehmen wir an, es sei das Integral
\begin{equation}
I=\int_{t_0}^{t_1} F(t, x(t)) \,dt
\end{equation}
zu minimieren. Ist $x(t)$ die gesucht Funktion, dann
wird das Integral $I$ f"ur jede der Funktionen $x(t)+s\delta(t)$
einen kleineren Wert haben. Es wird
\begin{equation}
\delta I =
\int_{t_0}^{t_1}F(t, x(t) + s\delta(t))\,dt
-
\int_{t_0}^{t_1}F(t, x(t))\,dt\le 0
\end{equation}
sein f"ur jede Wahl von $\delta(t)$ und von $s$. Insbesondere wird die
Ableitung nach $s$ an der Stelle $s=0$ eine Nullstelle haben
{\it f"ur jedes $\delta$}:
\begin{equation}
\frac{d}{ds}\int_{t_0}^{t_1}F(t,x(t)+s\delta(t))\,dt\;\bigg|_{s=0}=0
\end{equation}
Da die Integrationsgrenzen nicht von $s$ abh"angen, kann man die
Ableitung unter das Integral ziehen:
\begin{align*}
\int_{t_0}^{t_1}\frac{d}{ds}F(t, x(t)+s\delta(t))\,dt\;\bigg|_{s=0}
=
\int_{t_0}^{t_1}\frac{\partial F(t, x(t))}{\partial x}\delta(t)\,dt=0
\end{align*}
Auf den ersten Blick sieht das noch nicht nach einer Vereinfachung aus,
doch jetzt k"onnen wir die freie Wahlm"oglichkeit von $\delta$ ins
Spiel bringen.

\subsection{Eulergleichung zu einem Variationsproblem}
F"ur ein Variationsproblem der Form (\ref{variations-funktional})
l"asst sich die Variation relativ einfach durchf"uhren.
\begin{align}
0&=
\frac{d}{ds}
\int_{t_0}^{t_1} F(t, x(t) + s\delta(t), \dot x(t)+s\dot\delta(t))\,dt\bigg|_{s=0}
\notag
\\
&=
\int_{t_0}^{t_1} \frac{\partial F(t,x(t),\dot x(t))}{\partial x}\delta(t)
+\frac{\partial F(t,x(t),\dot x(t))}{\partial v}\dot \delta(t)\,dt
\notag
\\
&=
\int_{t_0}^{t_1} \frac{\partial F(t,x(t),\dot x(t))}{\partial x}\delta(t)\,dt
+\biggl[
\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\delta(t)
\biggr]_{t_0}^{t_1}
-\int_{t_0}^{t_1}\frac{d}{dt}\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\delta(t)\,dt
\notag
\\
&=
\int_{t_0}^{t_1}
\biggl(
\frac{\partial F(t,x(t),\dot x(t))}{\partial x}
-
\frac{d}{dt}\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\biggr)\delta(t)
\,dt
+\biggl[
\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\delta(t)
\biggr]_{t_0}^{t_1}
\label{euler-1}
\end{align}
Jetzt kann die Funktion $\delta(t)$ wieder frei gew"ahlt werden. 

Zum Beispiel kann man Funktionen w"ahlen, die an den Enden des Intervalles
verschwinden, also $\delta(t_0)=\delta(t_1)=0$.
F"ur solche Funktionen verschwindet der zweite Term in
(\ref{euler-1}). Da man $\delta(t)$ immer so w"ahlen kann, dass es
nur jeweils eine Komponenten in einer kleinen Umgebung einer eines
Punktes von $0$ verschieden ist, muss
der Integrand in (\ref{euler-1}) verschwinden.
Also muss die L"osungsfunktion die Differentialgleichung
\begin{equation}
\frac{d}{dt}\frac{\partial F}{\partial v}(t,x(t),\dot x(t))
-
\frac{\partial F}{\partial x}(t,x(t),\dot x(t))
\end{equation}
erf"ullen. Diese Differentialgleichung heisst die {\it Euler-Gleichung}
des Variationsproblems.

% falsch
%\subsection{Randbedingungen}
%Man kann in (\ref{euler-1}) aber auch Funktionen $\delta(t)$ w"ahlen, 
%die nur gerade einer Umgebung eines Endpunktes des Intervalls von
%$0$ verschieden sind. Daraus folgt dann, dass 
%\begin{equation}
%\frac{\partial F}{\partial v}(t_i,x(t_i),\dot x(t_i))=0
%\label{variation-randwerte}
%\end{equation}
%f"ur $i=0,1$ gelten muss. Dies ist eine Gleichung f"ur die Anfangswerte.

\subsection{Beispiele}
\subsubsection{K"urzeste Verbindung}
F"ur die k"urzeste Verbindung haben wir das Variationsprinzip
(\ref{kuerzeste-verbindung-variationsprinzip}) gefunden.
Die Funktion $F(t,x,v)=\sqrt{1+v^2}$ enth"alt die Variable
$x$ gar nicht, so dass die Eulergleichung nur einen Term
enthalten:
\begin{align}
\frac{d}{dt}\frac{\partial L}{\partial v}
&=\frac{d}{dt}\frac{2v}{2\sqrt{1+v^2}}\dot v =0
\notag
\\
&=\frac1{\sqrt{1+v^2}} - \frac{v^2}{(\sqrt{1+v^2})^3}\dot v
=\frac1{(1+v^2)^{\frac32}}\dot v=0
\label{kuerzester-abstand-gleichung}
\end{align}
Der Nenner in (\ref{kuerzester-abstand-gleichung}) ist immer
positiv, die Gleichung kann also nur erf"ullt werden, wenn
$\dot v =\ddot x=0$. Das ist gleichbedeutend damit, dass 
$x(t)$ eine lineare Funktion ist.
Die k"urzeste Verbindung zwischen zwei Punkten ist also eine
Gerade.

\subsubsection{Lagrange-Mechanik}
Die Eulergleichungen zur Variation der Lagrange-Funktion
(\ref{lagrange-funktion}) sind:
\begin{align}
\frac{d}{dt}\frac{\partial L}{\partial v}&=\frac{d}{dt}mv
\notag
\\
\frac{\partial L}{\partial x}&=\frac{V(x)}{\partial x}
\notag
\\
\Rightarrow\qquad
m\dot v&=-\frac{\partial V}{\partial x}
\label{lagrange-bewegungsgleichung}
\end{align}
Da der Gradient von $V$ auf der rechten Seite von
(\ref{lagrange-bewegungsgleichung}) die Kraft ist, erhalten wir als
Bewegungsgleichung
\begin{equation}
m\ddot x=F(x),
\end{equation}
also das erste Newtonsche Gesetz.


\section{Anwendungen}
\subsection{Mechanik}
\subsection{Interpolation}
\subsection{Brachistochrone}

