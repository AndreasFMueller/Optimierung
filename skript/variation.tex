\chapter{Variationsrechnung\label{chapter-variationsrechnung}}
\lhead{Variationsrechnung}

\section{Was ist Variationsrechnung?}
\rhead{}
Alle bisher betrachteten Optimierungsproblem gingen davon aus, dass
ein Vektor $x$, also endlich viele reelle Variablen, gefunden werden muss,
der eine Funktion $f(x)$ maximieren soll, aber in einem vorgegebenen
Gebiet von zul"assigen Vektoren liegen muss. 
Nat"urlich kann die Zahl der Variablen in so einem Problem sehr gross werden,
sie bleibt aber immer endlich.

In diesen Rahmen passt zum Beispiel das Problem, ein Polygon
maximalen Fl"acheninhaltes zu finden. Solange die Zahl der Ecken
des Polygons beschr"ankt bleibt, l"asst sich ein Polygon immer mit
endlich vielen Variablen beschreiben. Sucht man aber nicht nur in
der Menge der Polygone, sondern eine Kurve vorgegebener L"ange,
die maximalen Fl"acheninhalt einschliessen soll, dann kann man
die L"osung nicht mehr mit endlich vielen Variablen beschreiben.
Gesucht ist jetzt eine Funktion, nicht nur ein Vektor.

\subsection{Eine historische Herausforderung: Brachistochrone}
Im Juni 1696 ver"offentlichte Johann Bernoulli (1667-1748), der
zu dieser Zeit Professor f"ur Mathematik und Medizin in Groningen
war, den folgende Aufgabe in der Zeitschrift {\it Acta Eruditorum}:

\begin{quote}
Wenn in einer verticalen Ebene zwei Punkte $A$ und $B$ gegeben sind,
soll man dem beweglichen Punkte $M$ eine Bahn $AMB$ anweisen, auf welcher
er von $A$ ausgehend verm"oge seiner eigenen Schwere in k"urzester Zeit
nach $B$ gelangt.
\end{quote}

Offenbar wollte er auch etwas damit angeben, dass er das Problem
schon gel"ost hatte, und schrieb daher weiter
\begin{quote}
Damit Liebhaber solcher Dinge Lust bekommen sich an die L"osung
dieses Problems zu wagen, m"ogen sie wissen, dass es nicht, wie es
scheinen k"onnte, blosse Speculation ist und keinen praktischen
Nutzen hat. Vielmehr erweist es sich sogar, was man kaum glauben
sollte, auch f"ur andere Wissenszweige, als die Mechanik, sehr
n"utzlich. Um einem voreiligen Urtheile entgegenzutreten, m"oge noch
bemerkt werden, dass die gerade Linie $AB$ zwar die k"urzeste zwischen
$A$ und $B$ ist, jedoch nicht in k"urzester Zeit durchlaufen wird. Wohl
aber ist die Curve $AMB$ eine den Geometern sehr bekannte; die ich
angeben werde, wenn sie nach Verlauf dieses Jahres kein anderer
genannt hat.
\end{quote}
Leibniz reagiert postwendend mit einer L"osung, sein Antwortbrief tr"agt
das Datum 16. Juni 1696. Leibniz wies auch darauf hin, dass angesichts
der damals noch nicht so schnellen Postdienste die Antwortfrist noch
etwas verl"angert werden sollte.
Bernoulli publiziert daher im Januar 1697 nochmals einen nicht
minder schw"ulstigen Aufruf
\begin{quote}
Die scharfsinnigsten Mathematiker des ganzen Erdkreises gr"usst
Johann Bernoulli, "offentlicher Professor der Mathematik.
Da die Erfahrung zeigt, dass edle Geister zur Arbeit an der Vermehrung
des Wissens durch nichts mehr angetrieben werden, als wenn man ihnen
schwierige und zugleich n"utzliche Aufgaben vorlegt, durch deren
L"osung sie einen ber"uhmten Namen erlangen und sich bei der Nachwelt
ein ewiges Denkmal setzen, so hoffte ich den Dank der mathematischen
Welt zu verdienen, wenn ich nach dem Beispiele von M"annern wie
Mersenne, Pascal, Fermat, Viviani und anderen, welche vor mir
dasselbe thaten, den ausgezeichnetsten Analysten dieser Zeit eine
Aufgabe vorlegte, damit sie daran, wie an einem Pr"ufsteine, die
G"ute ihrer Methoden beurtheilen, ihre Kr"afte erproben und, wenn sie
etwas f"anden, mir mittheilen k"onnten; dann w"urde einem jeden
"offentlich sein verdientes Lob von mir zu Theil geworden sein.
\end{quote}
Im Mai-Heft 1697 der Acta Eruditorum wurden dann L"osungen von
Johann Bernoulli, seinem "alteren Bruder Jakob Bernoulli, von Marquis
de l'H\^opital (1661-1704) und
von Ehrenfried Walter Graf von Tschirnhausen (1651-1708)
ver"offentlicht.
Die L"osung von Leibniz wurde nicht ver"offentlich, Leibniz
meinte, seine L"osung sei so "ahnlich der L"osung der Bernoulli-Br"uder,
dass eine Publikation nicht nahel"age. Leibniz wies aber auch darauf hin,
dass Huygens, der 1695 verstorben war, das Problem sicher auch h"atte l"osen
k"onnen. Ebenso Newton.
Newton war mit der Analysis seit l"angerer Zeit im Besitz einer Methode,
mit der genau solche Aufgaben gel"ost werden konnten.
Allerdings ver"offentlichte er seine Methode erst 1704, was ihn
aber nicht daran hinderte, sie zum Beispiel zur L"osung des
Brachistochronen-Problems zu verwenden, die er anonym und unabh"angig von
den Acta Eruditorum in der Januar-Ausgabe der {\it Philosophical Transactions}.
Zu dieser Zeit galt Leibniz als der Erfinder der Infinitesimalrechnung,
mit der er sich seit 1670 besch"aftigt hatte.

Das Brachistochronen-Problem ist offenbar ein Variationsproblem.
Gesucht ist eine Kurve so, die von $A$ nach $B$ f"uhrt so, dass
die zum durchlaufen der Kurve n"otige Zeit m"oglichst kurz ist.

\subsection{Problemstellung}
Ein Variationsproblem ist also eine spezielle Art eines nichtlinearen
Optimierungsproblems, in dem nicht nur ein endlichdimensionaler
Vektor gefunden werden muss, sondern eine Funktion $[a,b]\to \mathbb R^n$.
Der linearen Zielfunktion $c^tx$ entspr"ache am ehesten ein Integral
\[
I(x)=\int_a^b c(t) x(t)\,dt.
\]
Wir haben aber im Kapitel \ref{chapter-lineare-optimierung} gesehen haben,
ist die L"osung eines linearen Optimierungsproblems vor allem eine 
Funktion des zul"assigen Bereichs.
Erst eine nichtlineare Zielfunktion f"uhrt zu einer interessanten
Theorie. Eine Zielfunktion der Form
\[
I(x)=\int_a^b F(t, x(t))\,dt.
\]
ist also angemessener.
Schon das Brachistochronen-Problem zeigt
jedoch, dass nicht nur die Funktionswerte, sondern auch die Werte
der Ableitung eine Rolle spielen k"onnen, und gelangen damit
zu Zielfunktion, die wir verwenden wollen:
\begin{equation}
I(x)=\int_a^b F(t, x(t), \dot x(t))\,dt,
\label{variations-funktional}
\end{equation}
wobei $F(t,x,v)$ eine stetige Funktion ist.

Grunds"atzlich spricht nichts dagegen, noch kompliziertere
Funktionen zuzulassen, die zum Beispiel auch noch h"ohere
Ableitungen ber"ucksichtigen.
Die Variationsproblem der Form (\ref{variations-funktional}) 
haben jedoch eine allgemeine L"osung, auf die wir im Folgenden
hinarbeiten wollen.

\subsection{Verallgemeinerungen}
In der bisherigen Formulierung des Variationsproblems war eine
Kurve $x(t)$ gesucht so, dass ein bestimmtes Funktional einen
minimalen Wert annimmt. Dies ist eine Einschr"ankung der bisherigen
Beispiele, nicht aber eine grunds"atzliche Einschr"ankung der Methode.

Sucht man zum Beispiel die Form einer Seifenhaut einer in einem
Rahmen eingespannten Seifenhaut, muss man offenbar eine Gleichung
aufstellen f"ur eine Funktion von zwei Parametern $\varphi(x,y)$,
definiert auf einem Definitionsgebiet $D\subset \mathbb R^2$ so, dass
die Werte von $\varphi$ auf dem Rande des Definitionsgebietes $D$ mit
den Punkten des Rahmens "ubereinstimmen. Die Seifenhaut nimmt eine
Form minimalen Fl"acheninhaltes an.

Weiter unten werden wir sehen, dass jedes Variationsproblem in eine
Differentialgleichung verwandelt werden kann.
Dies ist auch m"oglich f"ur das Minimalfl"achenproblem und die 
Funktion $\varphi(x,y)$.
Da jetzt allerdings
zwei Variablen vorhanden sind, nach denen abgeleitet werden kann, wird
sich eine partielle Differentialgleichung ergeben.
Wir beschr"anken uns im folgenden daher auf das eindimensionale
Variationsproblem.

\subsection{Beispiele}
\subsubsection{K"urzeste Verbindung}
Man finde die k"urzeste Verbindung zweier Punkte $x_0$ und $x_1$.
Offenbar muss hierzu die Kurvenl"ange
\begin{equation}
l(x)=\int_0^1 \sqrt{1+\dot x(t)^2}\,dt
\label{kuerzeste-verbindung-variationsprinzip}
\end{equation}
minimiert werden unter den Nebenbedingungen $x(0)=x_0$ und
$x(1)=x_1$.

\subsubsection{Isoperimetrisches Problem}
Gesucht ist eine Kurve gegebener L"ange in der Ebene, welche den
gr"osstm"oglichen Fl"acheninhalt einschliesst.
Der Sage nach soll K"onigin Dido auf der Flucht vor ihrem Bruder
Pygmalion bei Ihrer Ankunft in Nordafrika den dortigen Berberk"onig
Iarbas leihweise um ein St"uck Land gebeten haben nur so klein, dass man es mit
einer Ochsenhaut umz"aunen k"onne. Nach Iarbas' Zustimmung schnitt
Dido eine Ochsenhaut in einen langen Riemen und umz"aunte damit einen ganzen
H"ugel.

Etwas mathematischer, gesucht ist offenbar eine eine Funktion
$\gamma\colon [0,1]\to\mathbb R^2:t\mapsto \gamma(t)$. Die
L"ange muss einen ganz bestimmten Wert haben:
\[
\int_0^1 |\dot\gamma(t)|\,dt = l.
\]
Die eingeschlossene Fl"ache soll aber maximal sein:
\[
\frac12\int_0^1 x(t)\dot y(t)-y(t)\dot x(t)\,dt
\]
(Greensche Formel, siehe Vorlesung FuVar), wobei
\[
\begin{pmatrix}x(t)\\y(t)\end{pmatrix}=\gamma(t).
\]

"Aquivalent aber etwas "ubersichtlicher ist das folgende Problem.
Gegen sind zwei Punkte $A$ und $B$ in der Ebene. Gesucht ist eine
Kurve gegebener L"ange $l$ von $A$ nach $B$ so, dass die Fl"ache
zwischen der Kurve und der Strecke $AB$ maximalen Fl"acheninhalt hat.

Ohne Beschr"ankung der Allgemeinheit k"onnen wir annehmen, dass die
Punkte $A$ und $B$ symmetrisch bez"uglich des Nullpunktes auf der
$x$-Achse liegen, und dass die Kurve von $A$ nach $B$ sich durch eine
Funktion $y=f(x)$ beschreiben l"asst. Gesucht ist jetzt offenbar eine
Funktion $f(x)$ auf dem Intervall $[-a,a]$ so, dass die Fl"ache
\[
\int_{-a}^af(x)\,dx
\]
maximal wird, allerdings muss die L"ange der Kurve fest sein:
\[
\int_{-a}^{a} \sqrt{1+f'(x)^2}\,dx=l.
\]
Umgekehrt k"onnte man auch nach der Kurve minimaler L"ange
fragen, welche eine gegebene Fl"ache umfassen kann.

\subsubsection{Mechanik als Minimalproblem}
Vielen Naturgesetzen liegen Optimalit"atsprinzipien zu Grunde.
Das Brechungsgesetz kann man so formulieren: auf dem Weg $A$ 
nach $B$ nimmt Licht immer den schnellsten Weg.
Lagrange hat als erster erkannt, dass die Mechanik als Optimierungsproblem
verstanden werden kann. Doch welche Zielfunktion wird hier optimiert?

Wir m"ochten zum Beispiel die Bewegung eines Massepunktes in einem
Potential $V(x)$ verstehen, zum Beispiel die Bewegung eines Planeten
im Schwerefeld der Sonne.
Wir suchen also die Bahn $x(t)$ des Massepunktes finden. 
Wir erwarten, dass Position und Bewegungszustand des Massepunktes,
also $x(t)$ und $\dot x(t)$ wesentlich sein werden.
Wir vermuten also, dass es eine Funktion $L(t,x,v)$ gibt,
so dass das Integral
\begin{equation}
\int_{t_0}^{t_1} L(t, x(t), \dot x(t))\,dt
\label{lagrange-integral}
\end{equation}
durch die tats"achliche Bahnkurve von $x(t_0)$ nach $x(t_1)$
minimiert wird.

Intuitiv wissen wir, dass eine stabile Lage eines mechanischen Systems
sich durch minimale Energie auszeichnet: befindet sich ein System nicht
im Zustand minimaler Energie, kann man es durch eine kleine 
St"orung m"oglicherweise dazu bringen, seinen Zustand zu "andern
und dabei Energie abzugeben. Allerdings kann die Energie eines
einzelnen Massepunktes diese Funktion nicht "ubernehmen, da sie
konstant ist. Das Prinzip, dass ein System den Zustand kleinster
Energie annimmt, ist genauer betrachtet eine Bedingung an die potentielle
Energie. Je kleiner die potentielle Energie, und je schneller dieser
Zustand erreicht wird, desto besser. Als Extremalprinzip bietet
sich daher an dass die Differenz von potentieller und kinetischer
Energie minimiert werden soll. Wir setzen also
\begin{equation}
L(t,x,v)=V(x)-\frac12mv^2.
\label{lagrange-funktion}
\end{equation}
Diese Funktion $L(t,x,v)$ heisst Lagrange-Funktion des mechanischen
Systems, wir
verwenden sie im Funktional (\ref{lagrange-integral}).

\subsubsection{Interpolation}
F"ur die Berechnung einer Funktion $f(x)$ auf einem Interval
$[a,b]$ mit dem Computer wird oft eine Interpolationsverfahren verwendet.
Einige Funktionswerte $y_k=f(x_k)$, $a=x_0<x_1<\dots <x_n = b$,
einer Funktion sind exakt bekannt,
die Werte $f(x)$ f"ur die dazwischen liegenden $x$-Werte sollen
mit einer einfach zu berechnenden N"aherungsformel $g(x)$ berechnet
werden. 

Nat"urlich gibt es unendlich viele Funktionen $g(x)$, welche mit der
Funktion $f(x)$ in den Punkten $x_k$ "ubereinstimmt.
Es braucht also ein Kriterium, nach dem man eine ``m"oglichst gute''
Interpolationsfunktion $g(x)$ ausw"ahlen kann.

Eine erfolgreiche L"osung geht von einer Idee aus dem Schiffsbau aus:
man stellt sich die Interpolationsfunktion als eine Holzlatte vor, die
in den Punkten $(x_k,y_k)$ drehbar gelagert ist. Die Holzlatte wird
eine Form annehmen, bei der die Spannung in der Latte minimal ist.
Als Mass f"ur die Spannung kann man die zweite Ableitung der Funktion $g(x)$
nehmen. Gesucht ist also eine Funktion $g(x)$, die das Integral
\[
\int_a^b [g''(x)]^2\,dx
\]
minimiert, und in den Punkten $x_k$ den ``richtigen'' Funktionswert 
$g(x_k)=f(x_k)$ annimmt. Ausserdem soll die Funktion einmal stetig 
differenzierbar sein.

Die L"osung dieses Variationsproblems f"uhrt auf die sogenannte
Spline-Interpolation, die zum Beispiel in der Computer-Graphik oder
in der Wegplanung f"ur Roboter beliebt sind.

\section{Euler-Gleichungen}
\rhead{Euler-Gleichungen}
Es gibt kaum Verfahren, ein allgemeines Variationsproblem zu l"osen.
Aber es gibt ein allgemeines Vorgehen, wie man ein Variationsproblem
in eine Differentialgleichung umwandeln kann. F"ur Differentialgleichungen
steht ja eine grosses Arsenal von L"osungstechniken.
\subsection{Variation}
Das Maximum einer Funktion findet man, indem man diejenigen
Punkte sucht, in deren unmittelbarer Umgebung die Funktion kleinere
Werte annimmt. Erkennen kann ma solche Punkte daran, dass die Ableitung
der Funktion dort verschwindet.

"Ubertragen auf das Variationsproblem muss man also Funktionen 
suchen, die der L"osungsfunktion $x(t)$ benachbart sind.
Eine benachbarte Funktion kann zum Beispiel von der Form
$x(t)+s\delta(t)$ sein, wobei wir uns die Funktion $\delta(t)$
als kleine Abweichung vorstellen, die wir mit dem Parameter
$s$ noch kleiner machen k"onnen. 

Nehmen wir an, es sei das Integral
\begin{equation}
I=\int_{t_0}^{t_1} F(t, x(t)) \,dt
\end{equation}
zu minimieren. Ist $x(t)$ die gesucht Funktion, dann
wird das Integral $I$ f"ur jede der Funktionen $x(t)+s\delta(t)$
einen kleineren Wert haben. Es wird
\begin{equation}
\delta I =
\int_{t_0}^{t_1}F(t, x(t) + s\delta(t))\,dt
-
\int_{t_0}^{t_1}F(t, x(t))\,dt\le 0
\end{equation}
sein f"ur jede Wahl von $\delta(t)$ und von $s$. Insbesondere wird die
Ableitung nach $s$ an der Stelle $s=0$ eine Nullstelle haben
{\it f"ur jedes $\delta$}:
\begin{equation}
\frac{d}{ds}\int_{t_0}^{t_1}F(t,x(t)+s\delta(t))\,dt\;\bigg|_{s=0}=0
\end{equation}
Da die Integrationsgrenzen nicht von $s$ abh"angen, kann man die
Ableitung unter das Integral ziehen:
\begin{align*}
\int_{t_0}^{t_1}\frac{d}{ds}F(t, x(t)+s\delta(t))\,dt\;\bigg|_{s=0}
=
\int_{t_0}^{t_1}\frac{\partial F(t, x(t))}{\partial x}\delta(t)\,dt=0
\end{align*}
Auf den ersten Blick sieht das noch nicht nach einer Vereinfachung aus,
doch jetzt k"onnen wir die freie Wahlm"oglichkeit von $\delta$ ins
Spiel bringen.

\subsection{Eulergleichung zu einem Variationsproblem}
F"ur ein Variationsproblem der Form (\ref{variations-funktional})
l"asst sich die Variation relativ einfach durchf"uhren.
\begin{align}
0&=
\frac{d}{ds}
\int_{t_0}^{t_1} F(t, x(t) + s\delta(t), \dot x(t)+s\dot\delta(t))\,dt\bigg|_{s=0}
\notag
\\
&=
\int_{t_0}^{t_1} \frac{\partial F(t,x(t),\dot x(t))}{\partial x}\delta(t)
+\frac{\partial F(t,x(t),\dot x(t))}{\partial v}\dot \delta(t)\,dt
\notag
\\
&=
\int_{t_0}^{t_1} \frac{\partial F(t,x(t),\dot x(t))}{\partial x}\delta(t)\,dt
+\biggl[
\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\delta(t)
\biggr]_{t_0}^{t_1}
-\int_{t_0}^{t_1}\frac{d}{dt}\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\delta(t)\,dt
\notag
\\
&=
\int_{t_0}^{t_1}
\biggl(
\frac{\partial F(t,x(t),\dot x(t))}{\partial x}
-
\frac{d}{dt}\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\biggr)\delta(t)
\,dt
+\biggl[
\frac{\partial F(t,x(t),\dot x(t))}{\partial v}
\delta(t)
\biggr]_{t_0}^{t_1}
\label{euler-1}
\end{align}
Jetzt kann die Funktion $\delta(t)$ wieder frei gew"ahlt werden. 

Zum Beispiel kann man Funktionen w"ahlen, die an den Enden des Intervalles
verschwinden, also $\delta(t_0)=\delta(t_1)=0$.
F"ur solche Funktionen verschwindet der zweite Term in
(\ref{euler-1}). Da man $\delta(t)$ immer so w"ahlen kann, dass es
nur jeweils eine Komponenten in einer kleinen Umgebung einer eines
Punktes von $0$ verschieden ist, muss
der Integrand in (\ref{euler-1}) verschwinden.
Also muss die L"osungsfunktion die Differentialgleichung
\begin{equation}
\frac{d}{dt}\frac{\partial F}{\partial v}(t,x(t),\dot x(t))
-
\frac{\partial F}{\partial x}(t,x(t),\dot x(t))
=
0
\label{variation:euler-gleichung}
\end{equation}
erf"ullen. Diese Differentialgleichung heisst die {\it Euler-Gleichung}
des Variationsproblems.

\subsection{Variationsproblem mit Nebenbedingungen\label{variation:section-variationsproblem-nebenbedingungen}}
Das isoperimetrische Problem verlangt ein Funktional zu maximieren,
aber gleichzeitig muss ein anderes Funktional, die L"ange, konstant
zu halten. Etwas allgemeiner muss also ein Funktional
\[
I_F(x)=
\int_a^bF(t, x(t), \dot x(t))\,dt
\]
maximiert werden, w"ahrend gleichzeitig die Nebenbedingung
\begin{equation}
I_G(x)=
\int_a^b G(t, x(t), \dot x(t))\, dt = 0
\label{variation:nebenbedingung}
\end{equation}
gelten muss.

Wie in Abschnitt \ref{nlp:nebenbedingungen:mehrereariable} k"onnen wir
schliessen, dass bei einem Optimum die Richtungsableitungen
der beiden Funktionale proportional sind. Es muss also eine Zahl
$\lambda$ geben mit
\[
\delta I_F(x)=\lambda \delta I_G(x)
\]
Die gleiche Rechnung, die zu 
(\ref{nlp:nebenbedingungen:mehrereariable})
gef"uhrt hat, f"uhrt jetzt zu einer Gleichung
\begin{equation}
\frac{d}{dt}\frac{\partial F}{\partial v}(t,x(t),\dot x(t))
-
\frac{\partial F}{\partial x}(t,x(t),\dot x(t))
=
\lambdaÂ \left(
\frac{d}{dt}\frac{\partial G}{\partial v}(t,x(t),\dot x(t))
-
\frac{\partial G}{\partial x}(t,x(t),\dot x(t))
\right)
\end{equation}
oder 
\begin{equation}
\frac{d}{dt}\frac{\partial (F-\lambda G)}{\partial v}(t,x(t),\dot x(t))
-
\frac{\partial (F-\lambda G)}{\partial x}(t,x(t),\dot x(t))
=
0
\label{variation:euler-gleichung-Nebenbedingung}
\end{equation}
F"ur die L"osung des Variationsproblems mit Nebenbedingungen ist also
eine Funktion $x(t)$ und eine Zahl $\lambda$ zu finden, so
dass die Gleichung 
(\ref{variation:euler-gleichung-Nebenbedingung}) und
die Nebenbedingung
(\ref{variation:nebenbedingung})
erf"ullt.

\subsection{Beispiele}
\subsubsection{K"urzeste Verbindung}
F"ur die k"urzeste Verbindung haben wir das Variationsprinzip
(\ref{kuerzeste-verbindung-variationsprinzip}) gefunden.
Die Funktion $F(t,x,v)=\sqrt{1+v^2}$ enth"alt die Variable
$x$ gar nicht, so dass die Eulergleichung nur einen Term
enthalten:
\begin{align}
\frac{d}{dt}\frac{\partial L}{\partial v}
&=\frac{d}{dt}\frac{2v}{2\sqrt{1+v^2}}\dot v =0
\notag
\\
&=\frac1{\sqrt{1+v^2}} - \frac{v^2}{(\sqrt{1+v^2})^3}\dot v
=\frac1{(1+v^2)^{\frac32}}\dot v=0
\label{kuerzester-abstand-gleichung}
\end{align}
Der Nenner in (\ref{kuerzester-abstand-gleichung}) ist immer
positiv, die Gleichung kann also nur erf"ullt werden, wenn
$\dot v =\ddot x=0$. Das ist gleichbedeutend damit, dass 
$x(t)$ eine lineare Funktion ist.
Die k"urzeste Verbindung zwischen zwei Punkten ist also eine
Gerade.

\subsubsection{Isoperimetrisches Problem}
Das isoperimetrische Problem ist ein Variationsproblem mit
Nebenbedingungen.
Nach Abschnitt 
\ref{variation:section-variationsproblem-nebenbedingungen}
muss es zu einer L"osung des Problems eine Zahl $\lambda$
geben, so dass die Kurve $x(t)$ die Eulergleichung f"ur die Funktion
\[
F(t,x,v)-\lambda G(t,x,v)
\]
erf"ullt. Aus
\begin{align*}
F(t,x,v) &= x&
G(t,x,v) &= \sqrt{1+v^2}
\end{align*}
kann man die Eulergleichung f"ur die Funktion $H=F+\lambda G$
ableiten; zun"achst die partiellen Ableitungen von $H$
\begin{align*}
\frac{\partial H}{\partial x}&=1
&
\frac{\partial H}{\partial v}&=-\frac{\lambda v}{\sqrt{1+v^2}}
\end{align*}
und dann die Euler-Gleichungen
\begin{align*}
\frac{d}{dx} \frac{\lambda f'(x)}{\sqrt{1+f'(x)^2}}-1&=0
\\
\frac{d}{dx} \frac{\lambda f'(x)}{\sqrt{1+f'(x)^2}}&=1
\\
\frac{\lambda f'(x)}{\sqrt{1+f'(x)^2}}&=x - a
\\
\frac{f'(x)^2}{1+f'(x)^2}&=\frac{(x-a)^2}{\lambda^2}
\end{align*}
Unter Verwendung von
\[
\frac{X}{1+X}=Y
\qquad
\Rightarrow
\qquad
X=\frac{Y}{1-Y}
\]
wird aus der letzten Gleichung die Differentialgleichung
\begin{align*}
f'(x)^2&=\frac{(x-a)^2}{\lambda^2-(x-a)^2}
\\
f'(x)&=\frac{x-a}{\sqrt{\lambda^2-(x-a)^2}}
\end{align*}
f"ur die Funktion $f(x)$. Das Integral kann man durchf"uhren, 
Mathematica findet zum Beispiel
\[
y=f(x)=\sqrt{\lambda^2 - (x-a)^2}+b
\]
oder
\[
(x-a)^2+(y-b)^2=\lambda^2,
\]
die Gleichung eines Kreises mit Mittelpunkt $(a,b)$ und Radius
$\lambda$.
Damit haben wir gezeigt, dass die optimale L"osung f"ur das
isoperimetrische Problem ein Kreisbogen sein muss, und die
Bedeutung des Lagrange-Multiplikators ist jetzt auch klar, es
ist der Radius des Kreises.


\subsubsection{Lagrange-Mechanik}
Die Eulergleichungen zur Variation der Lagrange-Funktion
(\ref{lagrange-funktion}) sind:
\begin{align}
\frac{d}{dt}\frac{\partial L}{\partial v}&=\frac{d}{dt}mv
\notag
\\
\frac{\partial L}{\partial x}&=\frac{V(x)}{\partial x}
\notag
\\
\Rightarrow\qquad
m\dot v&=-\frac{\partial V}{\partial x}
\label{lagrange-bewegungsgleichung}
\end{align}
Da der Gradient von $V$ auf der rechten Seite von
(\ref{lagrange-bewegungsgleichung}) die Kraft ist, erhalten wir als
Bewegungsgleichung
\begin{equation}
m\ddot x=F(x),
\end{equation}
also das erste Newtonsche Gesetz.

\subsection{Anwendungen}
\subsubsection{Spline-Interpolation}
Das Interpolationsproblem verwendet das Funktional
\begin{equation}
I(g)=
\int_a^b[g''(x)]^2\,dx,
\label{variation:spline-funktional}
\end{equation}
welches nicht von der Art (\ref{variations-funktional}) ist.
Es ist daher auch nicht m"oglich, die Euler-Gleichung anzuwenden.
Aber die Methode der Variation, mit der die Euler-Gleichung gefunden
wurde, ist auch in diesem Beispiel anwendbar, wir m"ussen die gleiche
Ableitung nochmals durchf"uhren.
Wir berechnen also die Variation
des Funktionals
(\ref{variation:spline-funktional})
f"ur ein einziges Teilinterval $[t_k,t_{k+1}]$
\begin{align*}
0&=
\left.
\frac{d}{ds}\int_{t_k}^{t_{k+1}} [g''(x)+s\eta''(x)]^2\,dx
\right|_{s=0}
\\
&=
\int_{t_k}^{t_{k+1}} 2(g''(x)+s\eta''(x))\eta''(x)\bigg|_{s=0}\,dx
\\
&=
\int_{t_k}^{t_{k+1}} 2g''(x)\eta''(x)\,dx
\\
&=
\left[g''(x)\eta'(x)\right]_{t_k}^{t_{k+1}}
-
\int_{t_k}^{t_{k+1}}
g'''(x)\eta'(x)\,dx
\\
&=
\left[g''(x)\eta'(x)\right]_{t_k}^{t_{k+1}}
-
\left[g'''(x)\eta(x)\right]_{t_k}^{t_{k+1}}
+
\int_{t_k}^{t_{k+1}} g^{(4)}(x)\eta(x)\,dx
\end{align*}
Da man $\eta(x)$ frei w"ahlen kann, auch so, dass
$\eta(t_k)=\eta(t_{k+1})=0$
und 
$\eta'(t_k)=\eta'(t_{k+1})=0$
folgt aus der letzten Gleichung, dass
\begin{equation}
g^{(4)}(x)=0
\label{variation:spline-euler}
\end{equation}
sein muss.
Die Gleichung 
(\ref{variation:spline-euler}) ist die Euler-Gleichung f"ur das Funktional
(\ref{variation:spline-funktional}).
Eine Funktion, deren vierte Ableitung verschwindet, ist notwendigerweise in
kubisches Polynom.
Die optimale Interpolationsfunktion muss also notwendigerweise auf jedem
Teilinterval $[t_k,t_{k+1}]$ ein Polynom dritten Grades sein.

Daraus l"asst sich jetzt eine Methode ableiten, mit der die optimale
Interpolationsfunktion gefunden werden kann.
Da dies jedoch nicht das eigentliche Thema dieses Skripts ist,
verzichten wir auf die Details.


