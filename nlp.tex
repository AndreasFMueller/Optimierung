\chapter{Nichtlineare Optimierung\label{chapter-nichtlineare-optimierung}}
Praktische Optimierungsprobleme sind oft nicht linear. 
Nicht nur die Zielfunktion kann nichtlinear sein, auch die
Beschreibung des zul"assigen Gebietes kann nichtlineare Ungleichungen
verwenden. 
\section{Extremwerte von Funktionen}
Das einfachste nichtlineare Optimierungsproblem verlangt nur die
Optimierung einer Zielfunktion, zul"assig ist ganz $\mathbb R^n$.
Es geht also darum, Maximum oder Minimum einer Funktion
\[
f\colon \mathbb R^n\to\mathbb R
\]
zu finden. Da der Definitionsbereich unberandet ist, ist ein
Extremum notwendigerweise ein innerer Punkt des Definitionsbereiches.

Beschr"ankungen des Definitionsgebietes f"uhren
zu zus"atzlichen Komplikationen, es muss jetzt unterschieden
werden, ob Extrema auf dem Rand oder doch im inneren des Definitionsgebietes
liegen.
Methoden, die auf Ableitungen basieren, die mit Grenzwerten
arbeiten, zum Beispiel Ableitungen, d"urfen nur noch einseitige
Grenzwerte verwenden.

\subsection{Funktionen einer Variablen}
Das einfachste nichtlineare Optimierungsproblem ist, das Maximum
oder Minimum einer Funktion $f\colon\mathbb R\to\mathbb R$ zu finden.
A priori ist nicht einmal klar, dass $f$ die verlangten Extrema
besitzt, nicht einmal wenn $f$ eine glatte Funktion ist. So hat
die Funktion
\[
x\mapsto e^{-x^2}
\]
ein Maximum im Punkt $x=0$, aber kein Minimum. Zwar kann jeder beliebig
kleine Werte $\varepsilon>0$ erreicht werden, zum Beispiel mit
$x=\sqrt{-\log \varepsilon}$, der Wert $0$ wird aber nicht angenommen.

Auch lassen sich f"ur das Extremum keine Kriterien angeben, die 
als Basis eines L"osungsverfahrens dienen k"onnten, wenn die Funktion
$f$ nicht minimale Glattheitseigenschaften hat.
Ist $f$ differenzierbar, dann muss in einem Extremum $x_0$
$f'(x_0)=0$ gelten. Nur wenn $f$ zweimal differenzierbar ist,
kann man auch noch entscheiden, ob $x_0$ eine lokale Extremalstelle
ist: $f''(x_0)>0$ zeigt ein lokales Minimum an, $f''(x_0) < 0$
ein lokales Maximum.

Ein nichtlineares Maximumproblem (Minimumproblem) f"ur eine differenzierbare
Funktion $f\colon\mathbb R\to\mathbb R$ wird also im Allgemeinen in
folgenden Schritten gel"ost:
\begin{enumerate}
\item Finde alle Nullstellen $\{x_1,\dots,x_n\}$ der Ableitung $f'(x)$.
\item Falls $f''(x_i)$ existiert, eliminiere alle $x_i$, f"ur die
$f''(x_i) > 0$ ($f''(x_i) < 0$).
gesucht wird.
\item Berechne alle Funktionswerte $y_i=f(x_i)$, und w"ahle den
gr"ossten (den kleinsten) aus.
\end{enumerate}

Ist die Funktion $f$ nicht "uberall differenzierbar, k"onnte sie Extrema
auch an Stellen annehmen, wo $f$ nicht differenzierbar ist. Daher
sind in Schritt~1 des obigen Algorithmus auch noch all jene Punkte
zu berÃ¼cksichtigen, in denen $f'(x)$ nicht definiert ist.

\subsection{Numerische L"osung}
Kandidaten f"ur die Extremalstellen einer Funktion sind also die
Nullstellen der Ableitung. Jedes Verfahren, welches Nullstellen
einer Funktion bestimmt, kann daher auch Kandidaten f"ur Extremalstellen
finden.

\subsubsection{Newton-Verfahren}
Das Newton-Verfahren kann aus einer angen"aherten L"osung $x_0$
mit Hilfe der Iteration
\begin{equation}
x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)}
\label{newton-1dim}
\end{equation}
eine verbesserte L"osung finden. Die Folge $x_n$ konvergiert mit
quadratischer Konvergenz gegen eine L"osung, wenn der Anfangswert
$x_0$ gen"ugend nahe bei einer L"osung liegt.

Das Newton-Verfahren konvergiert schlecht, wenn $f''(x_*)=0$.
Der kleine Nenner in (\ref{newton-1dim}) kann bewirken, dass
$x_{n+1}$ weit weg von $x_*$ zu liegen kommt, so dass die
Veraussetzung, dass $x_{n+1}$ bereits eine eingermassen gute 
Approximation sein muss, nicht mehr erf"ullt ist.

Da das Newton-Verfahren immer nur eine lokale Suche nach einer
Nullstelle von $f'(x)$ unternimmt, ist es grunds"atzlich nicht
in der Lage, alle lokalen Extrema zu finden.
Es kann nur bereits approximativ bekannte Nullstellen verbessern,
und h"ochstens in Ausnahmef"allen auch aus einer sehr schlechten Anfangssch"atzung eine gute L"osung finden.

\subsubsection{Intervallhalbierung}

\subsubsection{Sekantenverfahren}


\subsubsection{Beispiel}
XXX: TODO

\subsection{Funktionen mehrerer Variablen}
Hat eine Funktion mehrere Variablen
\[
f\colon\mathbb R^n\to\mathbb R:(x_1,\dots,x_n)\mapsto f(x_1,\dots,x_n)
\]
in einem Punkt $x_*=(x_{1*},\dots,x_{n*})$ ein Extremum,
dann hat auch die partielle Funktion
\[
x_i\mapsto f(x_{1*},\dots,x_i,\dots,x_{n*})
\]
ein Extremum, also muss ihre Ableitung in diesem Punkt
verschwinden. Die Ableitung der partiellen Funktion ist aber
die partielle Ableitung nach $x_i$, mithin m"ussen
alle partiellen Ableitungen von $f$ im Punkt $x_*$ verschwinden
\begin{equation}
\frac{\partial f}{\partial x_i}(x_{1*},\dots,x_{n*})=0\;\forall i,
\qquad \Leftrightarrow\qquad
\operatorname{grad}f=\nabla f=0.
\end{equation}

Ob in einem Punkt $x_*$ mit $\operatorname{grad}f(x_*)=0$ ein
Minimum oder Maximum vorliegt, l"asst sich aus der Taylor-Entwicklung
ableiten. In einer gen"ugend kleinen Umgebung von $x_*$ gilt
\begin{align*}
f(x)&=f(x_*)+\operatorname{grad}f(x_*)\cdot (x-x_*)+
\sum_{i,k=1}^n\frac{\partial^2 f(x_*)}{\partial x_i\partial x_k}(x_i-x_{i*})(x_k-x_{k*})+o((x-x_*)^2)
\\
&=f(x_*)+
\sum_{i,k=1}^n\frac{\partial^2 f(x_*)}{\partial x_i\partial x_k}(x_i-x_{i*})(x_k-x_{k*})+o((x-x_*)^2)
\end{align*}
Ein lokales Minimum liegt vor, wenn die Summe in einer Umgebung
von $x_*$ nur positive Werte annimmt, ein lokales Maximum liegt vor,
wenn sie nur negative Werte annimmt.
Die Matrix $H$
\begin{equation}
H=D^2f(x_*) \quad\text{mit Matrixelementen}\quad
h_{ik}=
\frac{\partial^2 f(x_*)}{\partial x_i\partial x_k},
\end{equation}
heisst {\it Hessische Matrix} von $f$.
\index{Hessische Matrix}
Mit ihr l"asst sich das Kriterium auch so formulieren: ein Minimum liegt vor,
wenn $\xi^t H\xi > 0$ gilt f"ur jeden Vektor $\xi\ne 0$.
\index{positiv definit}
Eine Matrix mit dieser Eigenschaft heisst {\it positiv definit}.
Ein lokales Maximum liegt vor, wenn $\xi^tH\xi<0$ f"ur $\xi\ne 0$,
in diesem Fall heisst $H$ {\it negativ definit}. Die Matrix 
\index{negativ definit}
\[
H=\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
\]
zeigt, dass eine Matrix weder positiv noch negativ definit zu sein
braucht.
F"ur stetig differenzierbare Funktionion ist die Hessische Matrix
symmetrisch, und ist daher diagonalisierbar.
Als einfaches Kriterium kann daher
verwendet werden, dass ein lokales Minimum genau dann vorliegt,
wenn alle Eigenwerte der Hessischen Matrix positiv sind.

\section{Extremwerte mit Nebenbedingungen}
\subsection{Funktionen einer Variablen}
Der zul"assige Bereich eines eindimensionalen Optimierungsproblems
ist immer ein Interval, schreiben wir $f\colon[a,b]\to\mathbb R$.
Extrema
liegen entweder im Inneren eines Intervals, und sind dort mit
Hilfe der Ableitung zu lokalisieren, oder sie sind in den
Endpunkten des Intervals zu finden.

W"ahrend bei einem inneren Punkt $x_0$ mit $f'(x_0)=0$
das Vorzeichen der zweiten Ableitung dar"uber entscheidet,
ob ein Maximum oder Minimum vorliegt, reicht dazu f"ur
Randpunkte die erste Ableitung.
Der linke Randpunkt $a$ ist nur dann ein lokales Minimum, wenn $f'(a)\ge 0$
ist.
Ebenso ist der rechte Randpunkt nur dann ein lokales Minimum wenn
$f'(a)\le 0$ ist.

Als Vorbereitung und Motivation der Kuhn-Tucker-Bedingungen
(\ref{kuhn-tucker}) des $n$-dimensionalen Falles, wollen
wir diese Beobachtung noch auf eine andere Weise formulieren.
Das Interval $[a,b]$ wird durch die zwei linearen Ungleichungen
\begin{equation}
\begin{aligned}
g_1(x)&=a-x\le 0\\
g_2(x)&=x-b\le 0
\end{aligned}
\label{nebenbedingungen-eindimensional}
\end{equation}
beschrieben.
F"ur ein lokales Minimum in $x$ haben wir folgende Bedingungen
abgeleitet:
\begin{equation}
f'(x)\quad
\begin{cases}
\le 0&\qquad\text{f"ur $x=a$}\\
=0&\qquad a<x<b\\
\ge 0&\qquad\text{f"ur $x=b$}
\end{cases}
\label{bedingungen-fuer-minimum}
\end{equation}
In den Randpunkten haben die Nebenbedingungen die Ableitung $g_1'(a)=-1$
und $g_2'(b)=1$.
Die Ableitung $f'(x)$ hat also in einem Ranbdpunkt entgegengesetzte
Vorzeichen wie die Ableitung der Funktion, die den Randpunkt definiert.
Es gibt also in jedem Fall Zahlen $\lambda_1\le 0$, $\lambda_2\le $,
die die Gleichung
\begin{equation}
f'(x)-\lambda_1g_1'(x)-\lambda_2g_2'(x)=0
\label{kuhn-tucker-eindimensional}
\end{equation}
erf"ullen.
Je nach Fall in (\ref{bedingungen-fuer-minimum}) k"onnen einzelne
der $\lambda_i$ von $0$ verschieden sein, gem"ass folgender Tabelle
\begin{center}
\begin{tabular}{|l|>{$}c<{$}|>{$}c<{$}|}
\hline
Fall&\lambda_1&\lambda_2\\
\hline
Minimum am linken Rand&\ge 0&=0\\
Extremum im Inneren&=0&=0\\
Minimum am rechten Rand&=0&\ge 0\\
\hline
\end{tabular}
\end{center}
F"ur jedes $i$ muss also mindestens eine der Gr"ossen $\lambda_i$
und $g_i(x)$ verschwinden, was man auch durch die
Bedingungen
\begin{equation}
\begin{aligned}
\lambda_1g_1(x)&=0\\
\lambda_2g_2(x)&=0
\end{aligned}
\label{kuhn-tucker-eindimensional-slack}
\end{equation}
ausdr\"ucken kann.

Mit den Bedinugungen (\ref{kuhn-tucker-eindimensional}) und
(\ref{kuhn-tucker-eindimensional-slack})
kann man also das Minimalproblem
durch Einf"uhrung zweier neuer Variablen $\lambda_1$ und $\lambda_2$
darauf zur"uckf"uhren, ein nichtlineares Gleichungssystem mit
drei Unbekannten $x$, $\lambda_1\ge 0$ und $\lambda_2\ge0$ und den
Gleichungen
\begin{equation}
\begin{aligned}
f'(x)-\lambda_1g_1'(x)-\lambda_2g_2'(x)&=0\\
\lambda_1g_1(x)&= 0\\
\lambda_2g_2(x)&= 0
\end{aligned}
\label{karush-kuhn-tucker-eindimensional}
\end{equation}
zu l"osen.
Die Gleichungen (\ref{karush-kuhn-tucker-eindimensional}) heissen
die Karush-Kuhn-Tucker-Bedingungen oder Kuhn-Tucker-Bedingungen.
Weiterhin erf"ullt sein m"ussen nat"urlich auch die
Nebenbedingungen (\ref{nebenbedingungen-eindimensional}).

So wie die Ableitung das Problem, ein Minimum zu finden, auf das
Problem reduziert, eine Gleichung zu l"osen, so reduziert die
eindimensionale Kuhn-Tucker-Bedingung (\ref{kuhn-tucker-eindimensional})
das Problem, eine Minimum
mit Nebenbedinungen zu finden, auf das Problem, eine L"osung der
Gleichungen (\ref{karush-kuhn-tucker-eindimensional}) zu finden.

\subsubsection{Beispiel}
Man finde ein Minimum der Funktion $f(x)=x^3-3x$ im Interval $[-2,2]$.

\medskip
{\parindent 0pt
Die Randbedingungen sind wieder (\ref{nebenbedingungen-eindimensional}).}
Das Gleichungssystem (\ref{karush-kuhn-tucker-eindimensional}) wird
zu
\begin{align*}
3x^2-3+\lambda_1-\lambda_2&=0\\
\lambda_1(-2-x)&= 0\\
\lambda_2(x-2)&= 0
\end{align*}
Aus der ersten Gleichung kann man ableiten:
\[
x=\pm
\sqrt{1-\frac{\lambda_1}3+\frac{\lambda_2}3}.
\]
F"ur $\lambda_1=\lambda_2=0$ findet man daraus die L"osungskandidaten
$x=\pm 1$. Da die zweite Ableitung $f''(x)=6x$ ist, gilt $f''(1)=6\ge 0$
und $f''(-1)=-6<0$, nur $x=1$ ist also ein lokales Minimum. Dies sind die
einzigen inneren lokalen Minima.

Ist $\lambda_1\ne 0$, muss zus"atzlich aus der zweiten
Gleichung $x=-2$ gelten, also
\begin{align*}
-2&=-\sqrt{1+\frac{\lambda_2}3}\\
4&=1+\frac{\lambda_2}3\\
\lambda_2&=9
\end{align*}
F"ur $\lambda_2=0$  folgt analog $x=2$ und
\begin{align*}
2&=\sqrt{1-\frac{\lambda_1}3}\\
4&=1-\frac{\lambda_1}3\\
\lambda_1&=-9
\end{align*}
Da aber $\lambda_1\le 0$ sein muss, ist $x=2$ kein lokales Minimum.
Es bleibt also nur noch, die beiden Kandidaten $x=-2$
mit $f(-2)=(-2)^3-3\cdot(-2)=-8+6=-2$
und $x=1$ mit $f(1)=-2$
zu untersuchen, wir haben also zwei Minima im Interval $[-2,2]$.
Man beachte, das das Minimum $x=-2$ von der Ableitung alleine nicht
gefunden wird.

\subsection{Funktionen mehrerer Variablen}
Mehrdimensionale Optimierungsprobleme erlauben jedoch
wesentlich kompliziertere zul"assige Gebiete. In diesem Abschnitt
sollen Gebiete betrachtet werden, die durch
eine Menge von m"oglicherweise nichtlinearen Gleichungen
\begin{equation}
g_j(x) = g_j(x_1,\dots,x_n) = 0, \quad 1\le j\le m,
\end{equation}
auch Nebenbedingungen genannt,
beschrieben sind. Man kann die $g_j$ auch als Komponenten einer
vektorwertigen Funktion
\begin{equation}
g\colon \mathbb R^n\to \mathbb R^m:(x_1,\dots,x_n)\mapsto (g_1(x),\dots, g_m(x)).
\end{equation}
auffassen.

Ein Minimum $x_*$ von $f$ muss nicht mehr die Bedingung
$\operatorname{grad}f(x_*)=0$ erf"ullen. Diese Bedingung gibt ja
wieder, dass jede Ver"anderung von $x_*$ einen gr"osseren Wert
liefern wird. Dies muss jetzt nur noch erf"ullt sein f"ur
Ver"anderungen, die tangential an die durch die Gleichungen
$g_j(x)=0$ definierten Fl"achen erfolgen. 

Ein Vektor $\xi$ ist genau dann tangential an alle Fl"achen $g_j(x)=0$
im Punkt $x_*$,
wenn er auf dem Normalenvektor $\operatorname{grad}g_j(x_*)$
senkrecht steht, wenn also
\begin{equation}
\operatorname{grad}g_j(x_*)\cdot \xi=0.
\end{equation}
Die Richtungsableitungen von $f$ in Richtung $\xi$ muss f"ur jedes solche
$\xi$ verschwinden, also
\begin{equation}
\operatorname{grad}f(x_*)\cdot\xi = 0.
\end{equation}
Der Vektor $\operatorname{grad}f(x_*)$ muss also im Vektorraum
aufgespannt von den Vektoren $\operatorname{grad}g_j(x_*)$ liegen,
es muss also Zahlen $\lambda_j\in\mathbb R$ geben mit
\begin{equation}
\operatorname{grad}f(x_*)-\sum_{j=1}^m\lambda_j \operatorname{grad}g_j(x_*).
\end{equation}
Das Optimierungsproblem 
\begin{align*}
\text{Maximiere}\quad f(x)&=0\\
\text{unter Nebenbedingungen}\quad g_j(x)&=0\forall j
\end{align*}
kann daher mit folgendem Algorithmus gel"ost werden:
\begin{enumerate}
\item Finde L"osungen $x$, $\lambda_j$ des Gleichungssystems
\begin{align*}
g_j(x)&=0\quad\forall j\\
\operatorname{grad}f(x)-\sum_{j=1}^m\lambda_j\operatorname{grad}g_j(x)&=0
\end{align*}
Dies ist ein nichtlineares Gleichungssystem mit $m+n$ Gleichungen 
f"ur die $m+n$ Unbekannten $x_1,\dots,x_n,\lambda_1,\dots,\lambda_m$.
\item 
Test durch Auswerten von f(x) f"ur alle im ersten Schritt gefundenen
Kandidaten, welche zu einem Minimum f"uhren.
\end{enumerate}

\section{Nichtlineare Ungleichungen}
Dass zul"assige Gebiet eines
nichtlinearen Optimierungsproblems kann ausserdem durch 
nichtlineare Ungleichungen der Form
\begin{equation}
h_k(x) = h_k(x_1,\dots,x_n) \le 0,\quad 1 \le k\le l
\end{equation}
eingeschr"ankt werden.
Extrema von $f$ k"onnen sowohl im Inneren des zul"assigen Gebietes
wie auch auf dem Rand liegen.
Dabei k"onnen einige der Ungleichungen auch scharf erf"ullt
sein, also als Gleichungen. Der Rand des zul"assigen Gebietes
wird also in Kurven, Fl"achenst"ucke und Punkte aufgeteilt, je nachdem
wie viele der Ungleichungen scharf erf"ullt sind.

Im Gegensatz zum linearen Optimierungsproblem muss aber ein
Optimum auf dem Rand nicht unbedingt in einer Ecke liegen,
sondern kann auch isoliert auf einem Kurven- oder Fl"achenst"uck
des Randes auftreten.
Nehmen wir also an, ein Minimum werde im Punkt $x_*$ angenommen,
und sei $h_k(x_*)=0$ eine Ungleichung, die scharf erf"ullt ist.
Richtungen $\xi\in\mathbb R^n$, f"ur die
$\operatorname{grad}h_j(x_*)\cdot\xi > 0$ ist,
f"uhren aus dem zul"assigen Gebiet hinaus. Es d"urfen also nur Richtungen
$\xi$ betrachtet werden, f"ur die $\operatorname{grad}h_j(x_*)\cdot\xi <0$
gilt.
Entfernt man sich vom Punkt $x_*$ in Richtung $\xi$, nimmt $f$
um den Betrag $\operatorname{grad}f(x_*)\cdot \xi$ zu.
Da $x_*$ minimal ist, muss $\operatorname{grad}f(x_*)\cdot\xi >0$
sein.

\section{Numerische Verfahren}
